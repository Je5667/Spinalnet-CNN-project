{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1616b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "550000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "first_HL = 256\n",
    "\n",
    "# Image preprocessing modules\n",
    "# Normalize training set together with augmentation\n",
    "tfs = []\n",
    "for i in range(1,4):\n",
    "    tfs.append(transforms.Compose([\n",
    "    transforms.RandomRotation([-15 * i,15*i]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "]))\n",
    "    \n",
    "    tfs.append( transforms.Compose([\n",
    "    transforms.ColorJitter(brightness = 0.05 * i, hue=0.05* i, saturation= 0.05* i),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "]))      \n",
    "    \n",
    "    tfs.append(transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4*i),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "    ]))\n",
    "\n",
    "\n",
    "tfs.append(transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "]))    \n",
    "\n",
    "\n",
    "\n",
    "# Normalize test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
    "])\n",
    "\n",
    "# CIFAR-100 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                   transform=transform_test)\n",
    "\n",
    "for tf in tfs:\n",
    "    trainset += torchvision.datasets.CIFAR10(root='./data',train=True,download=True,\n",
    "                                   transform=tf)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=200, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=200, shuffle=False, num_workers=0)\n",
    "\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99da987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenny\\AppData\\Local\\Temp\\ipykernel_15516\\776716191.py:190: UserWarning:\n",
      "\n",
      "nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "\n",
      "C:\\Users\\jenny\\AppData\\Local\\Temp\\ipykernel_15516\\776716191.py:89: UserWarning:\n",
      "\n",
      "nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of SRN18: 84.57 % (improvement)\n",
      "Test Accuracy of SRN34: 85.43 % (improvement)\n",
      "=============================================================\n",
      "Test Accuracy of SRN18: 87.4 % (improvement)\n",
      "Test Accuracy of SRN34: 87.8 % (improvement)\n",
      "=============================================================\n",
      "Test Accuracy of SRN18: 87.86 % (improvement)\n",
      "Test Accuracy of SRN34: 88.82 % (improvement)\n",
      "=============================================================\n",
      "Test Accuracy of SRN18: 89.34 % (improvement)\n",
      "Test Accuracy of SRN34: 89.64 % (improvement)\n",
      "=============================================================\n",
      "Epoch :4 Accuracy SRN18: (88.76%), Maximum Accuracy: 89.34%\n",
      "Test Accuracy of SRN34: 90.26 % (improvement)\n",
      "=============================================================\n",
      "Test Accuracy of SRN18: 90.81 % (improvement)\n",
      "Test Accuracy of SRN34: 90.55 % (improvement)\n",
      "=============================================================\n",
      "Epoch :6 Accuracy SRN18: (90.77%), Maximum Accuracy: 90.81%\n",
      "Test Accuracy of SRN34: 90.96 % (improvement)\n",
      "=============================================================\n",
      "Test Accuracy of SRN18: 90.93 % (improvement)\n",
      "Test Accuracy of SRN34: 91.14 % (improvement)\n",
      "=============================================================\n",
      "Test Accuracy of SRN18: 91.02 % (improvement)\n",
      "Epoch :8 Accuracy SRN34: (91.10%), Maximum Accuracy: 91.14%\n",
      "=============================================================\n",
      "Test Accuracy of SRN18: 91.11 % (improvement)\n",
      "Test Accuracy of SRN34: 92.21 % (improvement)\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block of ReseNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        \"\"\"Basic Block of ReseNet Builder.\"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # First conv3x3 layer\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "\n",
    "        #  Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "        # ReLU Activation Function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Second conv3x3 layer\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "\n",
    "        #  Batch Normalization\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "        # downsample for `residual`\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass of Basic Block.\"\"\"\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class SpinalResNet(nn.Module):\n",
    "    \"\"\"Residual Neural Network.\"\"\"\n",
    "\n",
    "    def __init__(self, block, duplicates, num_classes=10):\n",
    "        \"\"\"Residual Neural Network Builder.\"\"\"\n",
    "        super(SpinalResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 32\n",
    "        self.conv1 = conv3x3(in_channels=3, out_channels=32)\n",
    "        self.bn = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(p=0.02)\n",
    "\n",
    "        # block of Basic Blocks\n",
    "        self.conv2_x = self._make_block(block, duplicates[0], out_channels=32)\n",
    "        self.conv3_x = self._make_block(block, duplicates[1], out_channels=64, stride=2)\n",
    "        self.conv4_x = self._make_block(block, duplicates[2], out_channels=128, stride=2)\n",
    "        self.conv5_x = self._make_block(block, duplicates[3], out_channels=256, stride=2)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=4, stride=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #self.fc_layer = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, first_HL) #changed from 16 to 8\n",
    "        self.fc1_1 = nn.Linear(256 + first_HL, first_HL) #added\n",
    "        self.fc1_2 = nn.Linear(256 + first_HL, first_HL) #added\n",
    "        self.fc1_3 = nn.Linear(256 + first_HL, first_HL) #added\n",
    "        \n",
    "        self.fc_layer = nn.Linear(first_HL*4, num_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        # self.apply(initialize_weights)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal(m.weight.data, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_block(self, block, duplicates, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Create Block in ResNet.\n",
    "\n",
    "        Args:\n",
    "            block: BasicBlock\n",
    "            duplicates: number of BasicBlock\n",
    "            out_channels: out channels of the block\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential(*layers)\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(num_features=out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, duplicates):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of ResNet.\"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Stacked Basic Blocks\n",
    "        out = self.conv2_x(out)\n",
    "        out = self.conv3_x(out)\n",
    "        out = self.conv4_x(out)\n",
    "        out = self.conv5_x(out)\n",
    "        \n",
    "        \n",
    "        out1 = self.maxpool2(out)\n",
    "        #print('out1',out1.shape)\n",
    "        out2 = out1[:,:,0,0]\n",
    "        #print('out2',out2.shape)\n",
    "        out2 = out2.view(out2.size(0),-1)\n",
    "        #print('out2',out2.shape)\n",
    "        \n",
    "        x1 = out1[:,:,0,0]\n",
    "        x1 = self.relu(self.fc1(x1))\n",
    "        x2= torch.cat([ out1[:,:,0,1], x1], dim=1)\n",
    "        x2 = self.relu(self.fc1_1(x2))\n",
    "        x3= torch.cat([ out1[:,:,1,0], x2], dim=1)\n",
    "        x3 = self.relu(self.fc1_2(x3))\n",
    "        x4= torch.cat([ out1[:,:,1,1], x3], dim=1)\n",
    "        x4 = self.relu(self.fc1_3(x4))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        out = torch.cat([x, x4], dim=1)\n",
    "        \n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"Residual Neural Network.\"\"\"\n",
    "\n",
    "    def __init__(self, block, duplicates, num_classes=10):\n",
    "        \"\"\"Residual Neural Network Builder.\"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 32\n",
    "        self.conv1 = conv3x3(in_channels=3, out_channels=32)\n",
    "        self.bn = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(p=0.02)\n",
    "\n",
    "        # block of Basic Blocks\n",
    "        self.conv2_x = self._make_block(block, duplicates[0], out_channels=32)\n",
    "        self.conv3_x = self._make_block(block, duplicates[1], out_channels=64, stride=2)\n",
    "        self.conv4_x = self._make_block(block, duplicates[2], out_channels=128, stride=2)\n",
    "        self.conv5_x = self._make_block(block, duplicates[3], out_channels=256, stride=2)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=4, stride=1)\n",
    "        #self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.fc_layer = nn.Linear(256, num_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        # self.apply(initialize_weights)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal(m.weight.data, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_block(self, block, duplicates, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Create Block in ResNet.\n",
    "\n",
    "        Args:\n",
    "            block: BasicBlock\n",
    "            duplicates: number of BasicBlock\n",
    "            out_channels: out channels of the block\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential(*layers)\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(num_features=out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, duplicates):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of ResNet.\"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Stacked Basic Blocks\n",
    "        out = self.conv2_x(out)\n",
    "        out = self.conv3_x(out)\n",
    "        out = self.conv4_x(out)\n",
    "        out = self.conv5_x(out)\n",
    "        \n",
    "        out = self.maxpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ResNet(BasicBlock, [1,1,1,1]).to(device)\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2]).to(device)\n",
    "\n",
    "def SpinalResNet18():\n",
    "    return SpinalResNet(BasicBlock, [2,2,2,2]).to(device)\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3]).to(device)\n",
    "\n",
    "def SpinalResNet34():\n",
    "    return SpinalResNet(BasicBlock, [3, 4, 6, 3]).to(device)\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3]).to(device)\n",
    "\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr1 = learning_rate\n",
    "\n",
    "curr_lr2 = learning_rate\n",
    "\n",
    "\n",
    "\n",
    "model1 = SpinalResNet18().to(device)\n",
    "\n",
    "model2 = SpinalResNet34().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate) \n",
    "  \n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "best_accuracy1 = 0\n",
    "best_accuracy2 =0\n",
    "#%%\n",
    "mod1_his = []\n",
    "mod2_his = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model1(images)\n",
    "        loss1 = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        outputs = model2(images)\n",
    "        loss2 = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        \n",
    "    # Test the model\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        correct1 = 0\n",
    "        total1 = 0\n",
    "        correct2 = 0\n",
    "        total2 = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = model1(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total1 += labels.size(0)\n",
    "            correct1 += (predicted == labels).sum().item()\n",
    "            \n",
    "            outputs = model2(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total2 += labels.size(0)\n",
    "            correct2 += (predicted == labels).sum().item()\n",
    "    \n",
    "        mod1_his.append(correct1/total1)\n",
    "        mod2_his.append(correct2/total2)\n",
    "        if best_accuracy1> correct1 / total1:\n",
    "            curr_lr1 = learning_rate*np.ndarray.item(pow(np.random.rand(1),5))\n",
    "            update_lr(optimizer1, curr_lr1)\n",
    "            print('Epoch :{} Accuracy SRN18: ({:.2f}%), Maximum Accuracy: {:.2f}%'.format(epoch, \n",
    "                                              100 * correct1 / total1, 100*best_accuracy1))\n",
    "        else:\n",
    "            best_accuracy1 = correct1 / total1\n",
    "            print('Test Accuracy of SRN18: {} % (improvement)'.format(100 * correct1 / total1))\n",
    "            \n",
    "        if best_accuracy2> correct2 / total2:\n",
    "            curr_lr2 = learning_rate*np.ndarray.item(pow(np.random.rand(1),5))\n",
    "            update_lr(optimizer2, curr_lr2)\n",
    "            print('Epoch :{} Accuracy SRN34: ({:.2f}%), Maximum Accuracy: {:.2f}%'.format(epoch, \n",
    "                                              100 * correct2 / total2, 100*best_accuracy2))\n",
    "            print('=============================================================')\n",
    "            \n",
    "        else:\n",
    "            best_accuracy2 = correct2 / total2\n",
    "            print('Test Accuracy of SRN34: {} % (improvement)'.format(100 * correct2 / total2))\n",
    "            print('=============================================================')\n",
    "\n",
    "            \n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "d = {\"SpinalResNet18\" : mod1_his, \"SpinalResNet34\" : mod2_his}\n",
    "df = pd.DataFrame(d)\n",
    "fig = px.line(df, labels ={\n",
    "    \"index\" : \"epoch\",\n",
    "    \"value\" : \"accuracy\",\n",
    "    \"variable\" : \"models\"}, title = \"ResNet Accuracy by Epochs on 11X Augmentation\")\n",
    "fig.show()\n",
    "#torch.cuda.empty_cache() # PyTorch thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6330b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
